---
id: US-002
feature: FS-153
title: "Create and Optimize robots.txt"
status: completed
priority: P1
created: 2026-01-04
project: specweave-dev
---

# US-002: Create and Optimize robots.txt

**Feature**: [FS-153](./FEATURE.md)

---

## Acceptance Criteria

- [x] **AC-US2-01**: robots.txt file created in docs-site/static/ directory
- [x] **AC-US2-02**: Sitemap URL (https://spec-weave.com/sitemap.xml) referenced in robots.txt
- [x] **AC-US2-03**: Disallow rules added for non-public content (DOCUMENTATION-AUDIT files, _archive folders, internal-only paths)
- [x] **AC-US2-04**: Crawl-delay directives added for aggressive AI bots (GPTBot, CCBot) to prevent server overload
- [x] **AC-US2-05**: robots.txt accessible at https://spec-weave.com/robots.txt with HTTP 200 status (will be after deployment)
- [x] **AC-US2-06**: robots.txt syntax validates using Google Search Console robots.txt tester

---

## Implementation

**Increment**: [0153-documentation-site-seo-enhancements](../../../../increments/0153-documentation-site-seo-enhancements/spec.md)

**Tasks**: See increment tasks.md for implementation details.


## Tasks

_No tasks defined for this user story_
